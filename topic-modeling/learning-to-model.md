# Topicing Models
Following the principle that one doesn't understand anything one can't teach to another, I am going to explain topic modelling to myself until I know what the hell I'm doing.
Warning: I'll be blatantly copy-pasting without attribution to piece these ideas together. Internal use only. 

## My Big Explanation

### What Do?

#### Don't Know Categories

In the beginning, there is a corpus. You don't know anything about the categories/topics it contains.

Perhaps you make a vector-space model of this corpus. If so, you describe the vector space with a matrix.

The simplest vector-space model assigns each word a weight based on its term frequency (tf) in each document. Or, there are a lot of more complicated formulae to weight words, including probabilistic methods and tf*idf.

This vector-space model is then presented in a matrix with a column for each word and a row for each document. At the intersection of each word and document is the word's weight for that document. In this matrix, each document (row) is an N-dimensional vector, with N equal to the number of words (columns). The matrix itself only has two dimensions (rows and columns) but nonetheless describes an N-dimensional vector space.

N is too many dimensions, so you use algorithms to merge similar dimensions/columns. LSA, LSI, SVD, and PCA are all different algorithms that use linear algebra to collapse columns together.

Once you've collapsed several words/columns together, your new columns are mathematical constructs that share some similarities with categories. You don't know anything.

To find out what the categories are, what you want to do to discover categories is called "Clustering" or, to AI folks "Unsupervised Learning".  Basically, the raw vectors are compared to each other using some similarity metric (in vector space:  cosine similarity metric; in probability model, some probabilistic similarity metric). You group the most similar documents into clumps using one of many, many clustering algorithms (k-means is most popular).  These groups/clusters are now your categories.

Topics are not defined by the words; the topics/concepts/classes/categories are abstract ideas. They are represented by words.

#### Do Know Categories

In the beginning, there is a corpus, and an ontology. Your corpus documents belong to X number of ontology categories. You know what all these categories are, but don't know where they are in which documents. You need to instantiate your categories, by filling them up with words.

Perhaps you make a vector-space model of this corpus. If so, you describe the vector space with a matrix. This is still the same as if you didn't know the categories.

N is still too many dimensions. You want to reduce N to the X, number of predefined known categories. A human reads a small number of documents and categorizes them. An algorithm, such as SVM or (preferably) KNN, uses these sample documents to learn how to tell the categories apart. Then you can use it to categorize the rest of the documents.

Now you know how much of each document comes from each category. Your categories are more useful than the categories generated via method 1, because you picked them out yourself.

### Topic Modeling and Networks

####Inferring topic models from networks.
* McCallum et al. (2005). Author-Recipient-Topic (ART) Model. In ART, it is assumed that topics of letters, e-mails or direct messages between people can be inferred from knowledge of both the author and the recipient. Thus, ART takes into account the social structure of a communication network in order to generate topics.
* Dietz et al. (2007) created a model that looks at citation networks, where documents are generated by topical innovation and topical inheritance via citations.
* Nallapati et al. (2008) similarly creates a model that finds topical similarity in citing and cited documents, with the added ability of being able to predict citations that are not present.
* Blei himself joined the fray in 2009, creating the Relational Topic Model (RTM) with Jonathan Chang, which itself could summarize a network of documents, predict links between them, and predict words within them.
* Wang et al. (2011) created a model that allows for “the joint analysis of text and links between [people] in a time-evolving social network.” Their model is able to handle situations where links exist even when there is no similarity between the associated texts.

####Inferring networks from topic models.
(Using networks to visualize how documents or topics relate to one another)

Some models have been made that infer networks from non-networked text.
* Broniatowski and Magee (2010 & 2011) extended the Author-Topic Model, building a model that would infer social networks from meeting transcripts. They later added temporal information, which allowed them to infer status hierarchies and individual influence within those social networks.

Many times, however, rather than creating new models, researchers create networks out of topic models that have already been run over a set of data. Using networks, we can see how documents relate to one another, how they relate to topics, how topics are related to each other, and how all of those are related to words.
* Newton’s Chymistry project
* Elijah Meeks created a wonderful example combining topic models with networks in [Comprehending the Digital Humanities](https://dhs.stanford.edu/comprehending-the-digital-humanities/). Using fifty texts that discuss humanities computing, Elijah created a topic model of those documents and used networks to show how documents, topics, and words interacted with one another within the context of the digital humanities.
* [TopicNets](http://www.ics.uci.edu/~asuncion/pubs/TIST_11.pdf), a project that combines topic modeling and network analysis in order to create an intuitive and informative navigation interface for documents and topics. This is a great example of an interface that turns topic modeling into a useful scholarly tool, even for those who know little-to-nothing about networks or topic models.

Having a network with every document connected to every other document is scarcely useful, so generally we’ll make our decision such that each document is linked to only a handful of others. This allows for easier visualization and analysis, but it also destroys much of the rich data that went into the topic model to begin with. This information can be more fully preserved using other techniques, such as multidimensional scaling.

## What Have Other People Done?

[Topic Modeling Martha Ballard's Diary](http://historying.org/2010/04/01/topic-modeling-martha-ballards-diary/).
I took a lot of notes, and then accidentally deleted them.

Ted Underwood's 18th century "topic tree": [the tree](http://tedunderwood.com/18c-tree/), and [some explanation](http://tedunderwood.com/2011/04/04/revealing-the-relationships-between-topics-in-a-corpus/).
I took a lot of notes, and then accidentally deleted them.

## What Am I Trying To Do Here?

I have 208 plot summaries of novels. The summaries are roughly 1 to 5 paragraphs each, and stylistically sparse. I know the author gender and publication date for most of these novels.

The expecting topic-modeling-y questions would probably be:

1. What is the Gothic "about"? How do these topics change in prevalence over time? (Could probably be fluffed into a paper on its own if any trends emerge, but would work better in conjunction with something else.)

2. What books are the most "Gothic-y"? Or, for each thing that the Gothic is "about", what books are most representative of that thing? (If any of these are really unexpected, could be a paper on its own, with normal-English discussion of the books in question. If it's basically what people already think, would work better in conjunction with something else.)

So the normal thing to do would be to feed my plot summaries into Mallet, and use its results to answer 1 and 2 and make them a paper together. I have a lot of more complicated questions, though, that I am really interested in. The first three basically boil down to, "What does it look like if I sort everything into the two categories of...?"

3. **Male vs Female**. My preliminary research (just counting the relative prevalence of 70 motifs listed in the index of a bibliography) suggested there are lots of things that are mostly written about by men (torture, corpses, incest) but nothing that is mostly written about by women. Does that still hold? (There are several competing conceptions of the relative prevalence of the Male Gothic vs the Female Gothic, so just fact-checking the two genres' existence could be a paper regardless of the results.)

What does a very 'male' book look like? A very 'female' book? (If these match the ways people typically define the Male and Female Gothics, this would be good for a page or two in a paper arguing something else. If they don't match, this would be a paper all on its own, and a really interesting one.)

I have about six books for which no author information is known; based on the other documents, can I make guesses about them? (I won't be able to confirm this guess, but it will make for a good page or two of discussion either way, in a paper mostly about something else.)

4. **Horror vs. Terror**. These are the two genres of Gothic novel that scholars talk about. I could give you the horror/terror designations of maybe twenty novels (because I've read them) and then we can see if we can guess it for the others! (I can't imagine that these differences would be detectable from the plot summaries; the differences are largely stylistic, so this would probably have to wait until I had a corpus of full texts. It'll make a great paper then, though.)

5. **Novel vs. Chapbook**. Some of these plot summaries are of expensive multi-volume novels, and some are of cheap 72-page chapbooks. (Everything is either a novel or a chapbook.) I wanna ask all my men-vs-women questions again, this time about novels vs. chapbooks: are they different? How are they different? (This would also be a paper all by itself.)

Upon reflection, those might actually be asking "What happens if I sort everything into these two categories, and then map some kind of network?" The next three are definitely just network questions:

6. **Names**. A lot of names get re-used across stories. A lot of names are unlikely to show up in existing lists of names. (My favourite is a woman named Euthanasia. *Mary Shelley* named a character Euthanasia.) Can a program identify which words are names, and put them in a bucket for me? With only 208 books, I can also manually delete all the non-name words from each document, and feed that new corpus to a program to put the names in a bucket and count them. Can a program make me a network map of books that share character names? (This would be a paper by itself, though the more interesting the resulting network is, the better the paper will be.)

7. **Schools**. How well does the Gothic 'canon' match the large body of Gothic texts? Do books cluster in different 'schools'? If so, are these based around the acknowledged canon, or are there schools/influential books that the canon has overlooked? 

8. **Plagiarism**. It is well-accepted that the Gothic was marked by plagiarism, and not just in the creation of cheap abridgments of popular novels. Is it possible to see who was plagiarizing from whom, and how much?

Questions that I can already answer by myself in excel the way I did my paper on motifs:
* Do men or women write more chapbooks vs novels? How do chapbook/novel counts change over time? (A few paragraphs in a paper mostly about something else.)
* How long are most novels? How long are their chapters/ how many chapter do they have? Do these differences in form track to other differences, in author or time or subject matter?
* What time periods are most novels set in? What places? (These are questions a human will have to answer.) For each one, is it written about more by men or by women? How does the popularity of each one change over time? (If I do both time and place, probably a paper.)

It occurs to me, all the places where I say "paper" here, I really mean "chapter of my dissertation." Or "the entirety of my master's essay." 


## Works Read
[Topic Modeling and Network Analysis](http://www.scottbot.net/HIAL/?p=221) by Scott Weingart.
* Explains LSA and LDA.
* Gives examples of topic models from networks, and networks from topic models.

[LSA is a marvellous tool, but literary historians may want to customize it for their own discipline.](http://tedunderwood.com/2011/10/16/lsa-is-a-marvellous-tool-but-humanists-may-no-use-it-the-way-computer-scientists-do/) by Ted Underwood.
* Value of LSA is not its ability to identify synonyms (which is what information retrieval (i.e., mom) uses it for)
* avoid SVD because it compresses the matrix too much/in the wrong way, to find transitive kinds of association. --> highlights semantic relationships at the cost of slightly blurring other kinds of association
* if you’re interested in “topics” that are strictly semantic, you might want to use an algorithm that reduces dimensionality with SVD. If you’re interested in discourses, sociolects, genres, or types of diction, you might use LSA without dimensionality reduction.
* Ways of weighting:
    - For the normal LSA algorithm that uses dimensionality reduction, the consensus is that “log-entropy weighting” works well. You take the log of each frequency, and multiply the whole term vector by the entropy of the vector. I have found that this also works well for humanistic purposes.
    - For LSA without dimensionality reduction, I would recommend weighting cells by subtracting the expected frequency from the observed frequency. This formula “evens the playing field” between common and uncommon words — and it does so, vitally, in a way that gives a word’s absence from a long document more weight than its absence from a short one. (Not something I care about with plot summaries, because they're all basically the same length.)
    
[Tech note](http://tedunderwood.com/tech-notes/) by Ted Underwood.
* Vector space model works better than simple Pearson’s correlation, because the “cosine similarity” measure used in a vector space model automatically gives more weight to longer documents, and to documents where a term is very strongly represented. (I don't care about document length at the plot-summary stage. Do I care about documents where a term is very strongly represented? Will I want to give longer documents more weight later?)
* INSTEAD OF TF-IDF, because we're not interested in the rarest words but just the words themselves, assess frequency as the difference between the expected occurrence of a term and the actual number of occurrences in the document. -->  this formula: occurrences of X in Y – ((Y length/corpus length) * total occurrences of X in corpus)
* This means that some components of the vector are negative, which is actually important. Otherwise the fact that a word doesn’t occur in a book of 100,000 words would have the same weight as the fact that it doesn’t occur in a play of 15,000 words, because they would both “bottom out” at zero. (Definitely something that matters when I get to full texts.)
* Of course, the “topic trees” produced by this measure are only as good as the lists of words you feed into them. (Wait, at what stage did we feed in a list of words???????)
* It may be controversial whether or not to call this “topic modeling.” If you want to describe the internal structure of a literary work, this technique of course won’t do the job directly, because it doesn’t divide works into parts. But I think it does a pretty good job of identifying the implicit thematic structure of eighteenth-century discourse as a whole, and I wouldn’t be surprised if it turned out that the internal structure of individual works is defined in large part by the way these corpus-level topics weave in and out of them.
* I actually think I... don't want to weight novels by length? Since I don't think length influences the importance of the story to readers...? Udolpho and The Veiled Picture are both just 'a reading experience'? Or does Udolpho have a bigger impact...? I think I need to be very careful about what question I'm asking.

[Document Similarity Based on Concept Tree Distance](http://citeseer.uark.edu/projects/citeseerX/papers/HT2008_short_paper.pdf), by Susan Gauch.
* Concept tree distance is an alternative to vector space models
* Traditional vector space models (i.e. LSA?) turn keyword vectors into concept vectors (i.e. collapse columns??) to reduce semantic ambiguity. However, this ignores the parent-child relationships that the keywords have inside those concept vectors.
* In this study, we introduce a novel technique to construct concept trees representing documents and we apply the tree edit distance algorithm to calculate document similarity.
* Looks like a better way to identify whether documents are related to each other. Not useful if I want to find out what concepts my corpus is mostly about, potentially useful if I want to map a network of their relationships.

[Topic Modeling With the JAVA GUI + Gephi](http://electricarchaeology.ca/2011/11/11/topic-modeling-with-the-java-gui-gephi/) - Prepare your corpus of text, get topics with MALLET, prune the CSV, make a network, visualize it!
* Not actually a tutorial.

[Visualizing Structure in Topic Models](http://www.r-bloggers.com/visualising-structure-in-topic-models/)
* Argues that network maps aren't appropriate, points to a lot of other articles.

[The Digital Humanities Contribution to Topic Modeling](http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/)
* Describes what's gonna be in the journal... looks less focused on specific research case uses than I'd hoped, but still handy.

[Topic Modeling and Digital Humanities](http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/) by David M. Blei
* "Formally, a topic is a probability distribution over terms. In each topic, different sets of terms have high probability, and we typically visualize the topics by listing those sets (again, see Figure 1). As I have mentioned, topic models find the sets of terms that tend to occur together in the texts.[2] They look like “topics” because terms that frequently occur together tend to be about the same subject."
* "Traditionally, statistics and machine learning gives a “cookbook” of methods, and users of these tools are required to match their specific problems to general solutions. In probabilistic modeling, we provide a language for expressing assumptions about data and generic methods for computing with those assumptions."
* "In particular, LDA is a type of probabilistic model with hidden variables. Viewed in this context, LDA specifies a generative process, an imaginary probabilistic recipe that produces both the hidden topic structure and the observed words of the texts. ...Given a collection of texts, they reverse the imaginary generative process to answer the question “What is the likely hidden topical structure that generated my observed documents?”"
* "Probabilistic models beyond LDA posit more complicated hidden structures and generative processes of the texts. As examples, we have developed topic models that include syntax, topic hierarchies, document networks, topics drifting through time, readers’ libraries, and the influence of past articles on future articles." --> mom can point me towards a handy suite?
* "Here is the rosy vision. A humanist imagines the kind of hidden structure that she wants to discover and embeds it in a model that generates her archive. The form of the structure is influenced by her theories and knowledge — time and geography, linguistic theory, literary theory, gender, author, politics, culture, history. With the model and the archive in place, she then runs an algorithm to estimate how the imagined hidden structure is realized in actual texts. Finally, she uses those estimates in subsequent study, trying to confirm her theories, forming new theories, and using the discovered structure as a lens for exploration. She discovers that her model falls short in several ways. She revises and repeats." --> What hidden structure am I looking for...?
* *"A model of texts, built with a particular theory in mind, cannot provide evidence for the theory. (After all, the theory is built into the assumptions of the model.) Rather, the hope is that the model helps point us to such evidence."*
* "The goal is for scholars and scientists to creatively design models with an intuitive language of components, and then for computer programs to derive and execute the corresponding inference algorithms with real data. ...Probabilistic models promise to give scholars a powerful language to articulate assumptions about their data and fast algorithms to compute with those assumptions on large archives." --> So if I'm gonna get mom involved I need to start with a specific hypothesis, and model in a way that allows that hypothesis to be tested. I sort of already knew that, but I'll need to really focus on it.

[Topic Modeling and Figurative Language](http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody/) by LISA M. RHODY
* "LDA assumes that documents are like your neighbors’ baskets, and your neighbors are like authors who select from a limited number of available types of words in order to produce documents — in this case poems. Each author chooses to varying degrees how much of each kind of topic they use for each document; however, the number of total available topics, just like the total number of kinds of produce remains constant. While this constraint, the assumption that all the words in a corpus could be derived from a limited set of topics, strikes the human reader as an artificial limitation, it is a necessary constraint in order for LDA to work." --> Rhody chooses a genre of poetry which is assumed to be based on a small set of conventions, well suited to LDA; the Gothic is seen much the same way, also well-suited to LDA?
* **Should I use my Gothic 'canon' as training texts to identify the topics?** Or will this make it impossible to identify when books diverge interestingly from that canon?
* "Topic models (and LDA is one kind of topic modeling algorithm) are generative, unsupervised methods of discovering latent patterns in large collections of natural language text"
* "As Ian H. Witten, Eibe Frank, and Mark A. Hall remind us in Data Mining: Practical Machine Learning Tools and Techniques, the guiding factors for text mining generally and topic modeling specifically are to generate *actionable* and *comprehensible* results (9.5)."
* "methods for measuring the “interpretability of a topic model” (2). The authors present two human evaluation tests meant to discern the accuracy of models by using the keyword distributions ...and the topic to document probabilities ... — called word intrusion and topic intrusion tests respectively"
* "For the purposes of modeling poetry data, word intrusion would not be as effective a method for determining a model’s accuracy at categorizing documents or detecting latent patterns unless the specific changes that happen to the nature of topic distributions for poetic corpora are adjusted for. “Intruders” as individual words does not work for LDA topics of poetry because poems purposefully access and repurpose language in unexpected ways. In other words, topics from the models in my project were not easily interpreted by keywords alone, and yet the results are still useful."
* "My research confirms, to a degree, Ted Underwood’s suspicion that topics in literary studies are better understood as a representation of “discourse” (language as it is used and as it participates in recognized social forms) rather than a thematic string of coherent terms."
* "Topic models of poetry do not reflect the anecdotal evidence that LDA frequently leads to semantically meaningful word distributions. Instead, topic models of the Revising Ekphrasis dataset created four consistently recurring types of topics." --> 1, OCR and other language or dialect distinctive features; 2, Large “chunk” topics (Longer poems pull one or more topics toward language specific to them); 3, Semantically evident topics (but the illusion of thematic comprehensibility obscures what is actually being captured by the topic model; more accurate to say that Topics 32 and 54 participate in discourses surrounding “night” and “natural landscapes”); 4, Semantically opaque topics (her example didn't look opaque to me, but I'll accept the framing of "discourses of elegy" rather than "topic of death")
* Conclusion... isn't about ekphrasis at all. So, not a model I can take for what my own final project should try to look like-- but still a useful read overall, and something it could be useful to quote from.

## Works To-Read?
http://www.scottbot.net/HIAL/?p=39600
http://tedunderwood.com/2012/11/11/visualizing-topic-models/
http://sappingattention.blogspot.ca/

**[Journal of Digital Humanities](http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/)

[Getting Started with Mallet and Topic Modeling](http://electricarchaeology.ca/2011/08/30/getting-started-with-mallet-and-topic-modeling/) Links to sequel below

[EXTREMELY specific walkthrough of using mallet](http://programminghistorian.org/lessons/topic-modeling-and-mallet)

explain how the extensibility of LDA makes it quite a different kind of beast (relative to LSA): Learning author-topic models from text corpora. M Rosen-Zvi, C Chemudugunta, T Griffiths, P Smyth, M Steyvers, ACM Transactions on Information Systems (TOIS), ACM, 2010. http://www.datalab.uci.edu/papers/AT_tois.pdf

Closing out my to-read tabs:
http://tedunderwood.com/2011/03/17/a-selection-of-the-language-really-spoken-by-men/
http://tedunderwood.com/category/18c/
http://tedunderwood.com/category/methodology/
http://sappingattention.blogspot.ca/2011/01/clustering-from-search.html
http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/
https://ariddell.org/simple-topic-model.html
http://www.scottbot.net/HIAL/?p=19113

## Potential Resources
[David Mimno's Topic Modeling Bibliography](http://mimno.infosci.cornell.edu/topics.html)

[Chang’s implementation of LDA and related models in R](http://cran.r-project.org/web/packages/lda/index.html) Code package; currently completely incomprehensible.