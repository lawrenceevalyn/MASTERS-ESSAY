# Topicing Models
Following the principle that one doesn't understand anything one can't teach to another, I am going to explain topic modelling to myself until I know what the hell I'm doing.
Warning: I'll be blatantly copy-pasting without attribution to piece these ideas together. Internal use only. 

## My Big Explanation

### Algorithm Acronyms

tf-idf first creates a matrix of words and their weighted frequencies (weighted based on each word's rarity in the corpus, to avoid caring too much about "the"). Each word is a column of the matrix. tf-idf produces a matrix with one word per column, which isn't quite helpful yet.

LSA (LSI in CS) uses singular value decomposition to figure out how each word is related to every other word.  It looks at columns that are mathematically similar to each other and collapses them into one column. The term co-occurrences indicate, beneath the covers, some type of semantic relationship. The columns are also, somehow, dimensions in vector space.

This produces the all important "topics," which are just the resulting columns after a bunch of them have been collapsed.

SVD, when run with LSA, condenses the matrix & partially fuses related rows and columns — and as a result, the compressed matrix is able to measure transitive kinds of association. The words “gas” and “petrol” may rarely appear together. But they both appear with the same kinds of other words. A compressed matrix is better at identifying synonyms, and for that reason at information retrieval.

pLSA “discovers” a bunch of topics, attaches them to a list of words, and classifies the documents based on those topics. What if every document isn’t just a set of words, but a set of topics? In this model, our encyclopedia article about computing history might be drawn from several topics. --> groups words not just based on proximity, but also on topics. (Using, I guess, black magic.)

LDA improved upon this idea by turning it into a generative model of documents. (Takes the same three concept of words, topics, documents, instead of just generating topics from words, allows documents to be generated from topics. ?)

### Topic Modeling and Networks

####Inferring topic models from networks.
* McCallum et al. (2005). Author-Recipient-Topic (ART) Model. In ART, it is assumed that topics of letters, e-mails or direct messages between people can be inferred from knowledge of both the author and the recipient. Thus, ART takes into account the social structure of a communication network in order to generate topics.
* Dietz et al. (2007) created a model that looks at citation networks, where documents are generated by topical innovation and topical inheritance via citations.
* Nallapati et al. (2008) similarly creates a model that finds topical similarity in citing and cited documents, with the added ability of being able to predict citations that are not present.
* Blei himself joined the fray in 2009, creating the Relational Topic Model (RTM) with Jonathan Chang, which itself could summarize a network of documents, predict links between them, and predict words within them.
* Wang et al. (2011) created a model that allows for “the joint analysis of text and links between [people] in a time-evolving social network.” Their model is able to handle situations where links exist even when there is no similarity between the associated texts.

####Inferring networks from topic models.
(Using networks to visualize how documents or topics relate to one another)

Some models have been made that infer networks from non-networked text.
* Broniatowski and Magee (2010 & 2011) extended the Author-Topic Model, building a model that would infer social networks from meeting transcripts. They later added temporal information, which allowed them to infer status hierarchies and individual influence within those social networks.

Many times, however, rather than creating new models, researchers create networks out of topic models that have already been run over a set of data. Using networks, we can see how documents relate to one another, how they relate to topics, how topics are related to each other, and how all of those are related to words.
* Newton’s Chymistry project
* Elijah Meeks created a wonderful example combining topic models with networks in [Comprehending the Digital Humanities](https://dhs.stanford.edu/comprehending-the-digital-humanities/). Using fifty texts that discuss humanities computing, Elijah created a topic model of those documents and used networks to show how documents, topics, and words interacted with one another within the context of the digital humanities.
* [TopicNets](http://www.ics.uci.edu/~asuncion/pubs/TIST_11.pdf), a project that combines topic modeling and network analysis in order to create an intuitive and informative navigation interface for documents and topics. This is a great example of an interface that turns topic modeling into a useful scholarly tool, even for those who know little-to-nothing about networks or topic models.

Having a network with every document connected to every other document is scarcely useful, so generally we’ll make our decision such that each document is linked to only a handful of others. This allows for easier visualization and analysis, but it also destroys much of the rich data that went into the topic model to begin with. This information can be more fully preserved using other techniques, such as multidimensional scaling.

### What Do Computer Scientists Do?
Try to develop new ways to weight the words (i.e., alternatives to tf-idf)

Find new ways to match the documents to the categories, create new text classification algorithms (like LSA/LDA)
* simple vector space model in which the categories are represented by words in a vector space (one dimension per unique word in the whole vocabulary of the training documents) and the document is a vector and similiarity is defined as the angle between the vectors;  smaller angle, more similar
* probability based method

## Vocabulary
* tf-idf : a way to weight the words. Term Frequency: how many times the word appears. Inverse Document Frequency: to prevent top word from being "the," changes the previously-created term-document matrix to weight terms based on how rare they are in the overall set of documents.
* log-entropy weighting: an alternative to idf?
* LSA: vector-space model algorithm for merging columns of a term-document matrix. Uses cosine similarity?
* LDA
* MDS (Multidimensional Scaling)
* Type: word (in CS)
* Token: ocurrance of a word (in CS)

## What Am I Trying To Do Here?
I could, essentially, try to make my own new index of motifs, and then do all the same stuff to those motifs as I did to the originals.

I could also use the topics as a stepping-stone to try to map networks.

## Works Read
<!--List in order read; annotate -->
[Topic Modeling and Network Analysis](http://www.scottbot.net/HIAL/?p=221) by Scott Weingart.
* Explains LSA and LDA.
* Gives examples of topic models from networks, and networks from topic models.

[Mom email 1](./reference/topic-modeling-reference/mom01.txt).
* v basic topic modeling explanation (familiar)
* v basic breakdown of areas in which CS researchers work

[Mom email 2](./reference/topic-modeling-reference/mom02.txt).
* explains tf-idf and LSA/LSI in more detail.
* attachment: powerpoint on TresNet (irrelevant)

[LSA is a marvellous tool, but literary historians may want to customize it for their own discipline.](http://tedunderwood.com/2011/10/16/lsa-is-a-marvellous-tool-but-humanists-may-no-use-it-the-way-computer-scientists-do/) by Ted Underwood.
* Value of LSA is not its ability to identify synonyms (which is what information retrieval (i.e., mom) uses it for)
* avoid SVD because it compresses the matrix too much/in the wrong way, to find transitive kinds of association. --> highlights semantic relationships at the cost of slightly blurring other kinds of association
* if you’re interested in “topics” that are strictly semantic, you might want to use an algorithm that reduces dimensionality with SVD. If you’re interested in discourses, sociolects, genres, or types of diction, you might use LSA without dimensionality reduction.
* Ways of weighting:
    - For the normal LSA algorithm that uses dimensionality reduction, the consensus is that “log-entropy weighting” works well. You take the log of each frequency, and multiply the whole term vector by the entropy of the vector. I have found that this also works well for humanistic purposes.
    - For LSA without dimensionality reduction, I would recommend weighting cells by subtracting the expected frequency from the observed frequency. This formula “evens the playing field” between common and uncommon words — and it does so, vitally, in a way that gives a word’s absence from a long document more weight than its absence from a short one. (Not something I care about with plot summaries, because they're all basically the same length.)
    
[Tech note](http://tedunderwood.com/tech-notes/) by Ted Underwood.
* Vector space model works better than simple Pearson’s correlation, because the “cosine similarity” measure used in a vector space model automatically gives more weight to longer documents, and to documents where a term is very strongly represented. (I don't care about document length at the plot-summary stage. Do I care about documents where a term is very strongly represented? Will I want to give longer documents more weight later?)
* INSTEAD OF TF-IDF, because we're not interested in the rarest words but just the words themselves, assess frequency as the difference between the expected occurrence of a term and the actual number of occurrences in the document. -->  this formula: occurrences of X in Y – ((Y length/corpus length) * total occurrences of X in corpus)
* This means that some components of the vector are negative, which is actually important. Otherwise the fact that a word doesn’t occur in a book of 100,000 words would have the same weight as the fact that it doesn’t occur in a play of 15,000 words, because they would both “bottom out” at zero. (Definitely something that matters when I get to full texts.)
* Of course, the “topic trees” produced by this measure are only as good as the lists of words you feed into them. (Wait, at what stage did we feed in a list of words???????)
* It may be controversial whether or not to call this “topic modeling.” If you want to describe the internal structure of a literary work, this technique of course won’t do the job directly, because it doesn’t divide works into parts. But I think it does a pretty good job of identifying the implicit thematic structure of eighteenth-century discourse as a whole, and I wouldn’t be surprised if it turned out that the internal structure of individual works is defined in large part by the way these corpus-level topics weave in and out of them.
* I actually think I... don't want to weight novels by length? Since I don't think length influences the importance of the story to readers...? Udolpho and The Veiled Picture are both just 'a reading experience'? Or does Udolpho have a bigger impact...? I think I need to be very careful about what question I'm asking.

## Works To-Read?
[Ted Underwood](http://tedunderwood.com/category/methodology/topic-modeling/)

[tutorial on how to create networks using MALLET and Gephi quickly and easily.](http://electricarchaeology.ca/2011/11/11/topic-modeling-with-the-java-gui-gephi/) - Prepare your corpus of text, get topics with MALLET, prune the CSV, make a network, visualize it!

[Getting Started with Mallet and Topic Modeling](http://electricarchaeology.ca/2011/08/30/getting-started-with-mallet-and-topic-modeling/) Links to sequel below

[EXTREMELY specific walkthrough of using mallet](http://programminghistorian.org/lessons/topic-modeling-and-mallet)

[a great blog post about why humanists should avoid the SVD step of using LSA](http://tedunderwood.com/2011/10/16/lsa-is-a-marvellous-tool-but-humanists-may-no-use-it-the-way-computer-scientists-do/)

explain how the extensibility of LDA makes it quite a different kind of beast (relative to LSA): Learning author-topic models from text corpora. M Rosen-Zvi, C Chemudugunta, T Griffiths, P Smyth, M Steyvers, ACM Transactions on Information Systems (TOIS), ACM, 2010. http://www.datalab.uci.edu/papers/AT_tois.pdf

## Potential Resources
[David Mimno's Topic Modeling Bibliography](http://mimno.infosci.cornell.edu/topics.html)

[Chang’s implementation of LDA and related models in R](http://cran.r-project.org/web/packages/lda/index.html) Code package; currently completely incomprehensible.