On Tue, Jul 1, 2014 at 7:04 PM, Lawrence Evalyn
<lawrenceevalyn@gmail.com> wrote:
> I can't follow links that lead to your desktop!
>
> Ok, trying to connect your previous email and this one with what I've
> already started learning
>
> [Note: I wrote these stream-of-consciousness and there ended up being a lot;
> I'd appreciate it if you'd skim over them, and just respond to the parts
> where I seem to be on the wrong track.]
>
> From the last one--
> - Your 'vector space model' - is it related to LSA
In the vector space model, every document is a vector.
Consider 3 documents:
D1:  marriage and death in England
D2: marriage and death and birth in Germany and England
D3: marry in France
D4: cats and dogs in marriage

So, vocabulary is:  and, birth, cats, death, dogs, England, France,
Germany, in, marriage, marry
These are in alphabetical order, but order of the dimensions doesn't matter.
11 unique words, 11 dimensional vector space

In the simplest version, the weights are just tf (not tf*idf);
actually the Boolean model just uses 1 (it appeared) or 0 (it did not
appear).
D1 would be:  (1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0)
D2 would be:  (2, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0)
D3 would be:  (0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1)
D4 would be:  (1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0)

Each document is an 11-dimensional vector.
If you stuff the vectors in a 2d matrix, each row is a document and
each unique word is a column
(or vice versa, depending on the text book :-))

LSA takes these vectors and says:  Hm, there are not 11 dimensions,
lets do linear algebra to try to collapse this into fewer dimensions.
If the linear algebra worked properly on many, many documents, you can
picture one column of "marriage words", one of pets, one of countries,
and one of other (in, and, death).
So,
D1 would be: (1, 0, 1, 3)
D2 would be: (1, 0, 2, 6)
D3 would be: (1,0, 1, 1)
D4 would be: (1, 2, 0, 2)

Basically LSA takes a high dimensional vector space and maps it to a
lower dimensional vector spaces.  It does this by stuffing the vectors
into a matrix and doing SVD (singular value decomposition) on it to
try to identify independent dimensions (i.e., marriage and marry
belong together).



> - The 'probability based method' - that is LDA...? Which is 'Bayesian'?

When you represent the weight of a term in document, you can do better
than simply Boolean or tf.  Many formulae have been used:  Vector
space models tend to use tf * idf but you can also calculate the term
weights using probabilistic formulae.  Other models have also been
used.  But, in the end, you have a term x document matrix that stores
the weight of every term in every document.  That matrix goes into LSA
if you want to decrease the number of dimensions.

> - SVM - is this related to SVD? My understanding of SVD is that it is
> something one can run in conjunction with LSA, to make LSA better at
> identifying synonyms (better for information retrieval), but not something
> that a literary scholar should run, because we actually find it more useful
> to have 'rosseau' listed with 'sensibility' even though Rosseau is not
> actually a synonym.

SVD is just the linear algebra underneath LSA.  Neither really groups
exact terms.
When you decrease dimensions, the new column 3 might be 5% of old
column 6 + 82% of old column 8 + etc etc.  So, the new columns are NOT
exactly groups of words.  The new columns are quasi-related to
semantics, but there are no clear semantics associated with the
columns.

SVM is basically used when you KNOW the categories in advance; you
have sample documents for each category.  It learns how to tell the
categories apart.  Then, you can use it to categorize new documents.
It works best with only 2 categories (e.d, sports vs music).



>
> Also the question of 'a few' categories being <100 -- what dictates how many
> topics one should ask one's algorithm for?

It depends on our application.  For the web, I was using 3000+
categories.  For a collection of computer science documents, I have
400 categories.  For topic modeling or classification, you need to
start with the sets of categories (generally man made;  nowadays, we
call the sets of categories an ontology).  I steal mine from
elsewhere.  For the Web, I used Yahoo's categories; For computer
science documents, our professional society has a taxonomy of CS
topics.  For yours, you would want to grab a set of categories from
somewhere else.


>
> From this one--
> - About tf-idf -- if all my documents mention, for example, 'marriage',
> because they're all plot summaries of Gothic novels and almost all of them
> have marriages in them, tf-idf will weight 'marriage' weakly, like the way
> it weights 'the'? That seems... unhelpful...? If I wanted to use topic
> modeling to see what these books are mostly about, I'd want to know that all
> of them mentioned marriage.

Yes, but you would be getting your categories from ALL gothic novels,
or all novels.  Across that collection, marriage is rare.  In your
novels, it is common, therefore important.   You need to get idf from
a large collection.  When we were doing video, we had very little text
in the closed captions.  So, we used 2 years of the Wall Street
Journal or something to get the idfs.  Then used them on our small
texts from the closed captions which all said Valujet (the plane had
just crashed).

> - What tf-idf produces is just a matrix of words and their weighted
> frequencies, with no grouping into synonyms/topics? So (following off the
> idea that SVM does too much collapsing for literary scholars) I do have to
> run something to collapse some of the columns together?

tf-idf just produces 1 number - the weight of a single word in a
single document, based on word statistics in the document and across
the collection.

The Vector space (or even probabilistic model) represents each
document as a collection of term weights;  a vector or row in a
matrix.

> - LSA/LSI - the collapsing of columns together, that is "singular value
> decomposition"? If so, singular value decomposition definitely does not
> "figure out how each word is related to every other word" (as one of my DH
> articles glossed it), it has nothing to do with how, it just
> collapses/decomposes columns/words that it can already tell are
> mathematically closely related?

LSA/LSI/SVD/PCA (Principle Component Analysis) are all math that
collapses columns.  The columns are ROUGHLY semantic, but not exactly.

They are NOT topics... they are just weird math constructs that may or
may not group words together.

SVN is a way of deciding, for a new document, is about a topic or not.

What you want is what I do - conceptual representations of a document
that are semantically meaningful.

YOu start with your topics. You represent each topic by a bunch of
words.  You then say, if those words occur in the new document, then
it has that topic.

What I do is rather than MANUALLY assigning words to topics, I start
with text that is about that topic and then CALCULATE using a program
the weights of words in the document. Then I model the document by

Doc 7 is 33% topic 7 and 44% topic 3 and 18% topic 5.

My personalize search does this for people.  Send me your web pages
and I build a profile that is topical about what you are interested
in.
I can send papers...