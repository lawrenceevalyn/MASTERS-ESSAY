On Tue, Jul 1, 2014 at 9:53 PM, Lawrence Evalyn
<lawrenceevalyn@gmail.com> wrote:
> ### How To Topic Model
>
>
> #### Don't Know Categories
>
> In the beginning, there is a corpus. You don't know anything about the
> categories it contains.
>
> Perhaps you make a vector-space model of this corpus. If so, you describe
> the vector space with a matrix.
>
> The simplest vector-space model assigns each word a weight based on its
> Boolean term frequency (tf) in each document.

NOT Boolean - Boolean is just True/false; tf counts

 term frequency (tf) in each document.

> Or, there are a lot of more
> complicated formulae to weight words, including probabilistic methods and
> tf*idf.
>
> This vector-space model is then presented in a matrix with a column for each
> word and a row for each document. At the intersection of each word and
> document is the word's weight for that document. In this matrix, each
> document (row) is an N-dimensional vector, with N equal to the number of
> words (columns). The matrix itself only has two dimensions (rows and
> columns) but nonetheless describes an N-dimensional vector space.
>
> N is too many dimensions, so you use algorithms to merge similar
> dimensions/columns. LSA, LSI, SVD, and PCA are all different algorithms that
> use linear algebra to collapse columns together.
>

Good so far.

> Once you've collapsed several words/columns together, your new column is a
> category. Now you know what categories are in your corpus, and how much of
> each document comes from each category.

You are off the rails.  It is a big stretch to say that each column is
a category.
The columns are reduced, but they are not categories, they are just
weird mathematical constructs that share some similarities with
categories.

If you DON"T know categories, what you want to do to discover
categories is called "Clustering" or, to AI folks "Unsupervised
Learning".  Basically, the raw vectors are compared to each other
using some similarity metric (in vector space:  cosine similarity
metric; in probability model, some probabilistic similarity metric).
You group the most similar documents into clumps using one of many,
many clustering algorithms (k-means is most popular).  These
groups/clusters are now your categories.

>
> This is what most humanities researchers do with topic modeling, because
> they want to compare differences between texts or over time, or just see
> what words get lumped with other words. They call these categories "topics,"
> give these topics names like "midwifery" or "shopping," and then talk about
> how much somebody's diary mentions shopping over time.
>
>
> #### Do Know Categories
>
> In the beginning, there is a corpus. These documents belong to X number of
> categories. You know what all these categories are, but don't know where
> they are in which documents.
>
> Perhaps you make a vector-space model of this corpus. If so, you describe
> the vector space with a matrix. This is still the same as if you didn't know
> the categories.
>
> N is still too many dimensions. You want to reduce N to the X, number of
> predefined known categories. A human reads a small number of documents and
> categorizes them. An algorithm, such as SVM, uses these sample documents to
> learn how to tell the categories apart. Then you can use it to categorize
> the rest of the documents.
>
> Now you know how much of each document comes from each category. Your
> categories are more useful than the categories generated via method 1,
> because you picked them out yourself.
>
> Humanities researchers mostly don't do this, but they totally could.
>

You;ve got it.  I don't like SVN because it is poorly defined with
many categories.  I like knn (k-nearest neighbors)

> #### Questions
>
> At what stage does one opt for a concept-tree-distance model instead of a
> vector space model?
When one has an MS student who is looking for a thesis topic and then
when one wants to write a paper :-)