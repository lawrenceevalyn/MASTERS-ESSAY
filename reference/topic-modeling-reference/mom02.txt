Also read:
file:///Users/sgauch/Downloads/72e7e521f57ff12e09%20(1).pdf

What you need is basic Information Retrieval to learn about tf, idf.

Here is a talk I gave that is about it (TresNet).

I can explain LSA simply.  Basically, lets say you have 1000 documents.
Between them, they use 50,000 unique words (types) and 1,000,000 word
occurrences (tokens).

Simplest:
Build a 2d matrix of words cross documents that stores in each cell
the weight of that word in that documents.
That is the "term - document" matrix.
The simplest weight is the frequency of the term in the document.
BUt, then "the" would have a very high frequency.
So, we calculate idf (inverse document frequency) that calculates how
rare a term is in teh collection.  The fewer documents it occurs in,
the higher the idf.
So, wt = tf * idf    (high weighted terms are frequent in the document
but rare overall).

This matrix would be HUGE 50,000 words * 1,000 documents for a tiny collection.
10 million words * 1 trillion documents for Google.
Many elements are zero (most of the 50,000 words do not occur in a
given document).
So, search engines spend their time on algorithms to deal with sparse matrices.

Back to you - the 50,000 words (columns of the matrix) are not
independent of each other.  Dog and bark and fur are all in different
columns, but documents that have one of those words have the others
more frequently then randomness would predict.

LSA or, in my field, LSI - latent semantic indexing, is a linear
algebra approach.  Pure math.  It looks at columns that are similar to
each other and collapses them into one column.

So, Dog and bark and fur and cat and beagle go from five columns in
the original matrix to one column in the new smaller matrix.  There is
no "real" semantics going on - just math of patterns.  But, the
columns that collapse into each other have "latent" semantics.  The
term co-occurrences indicate, beneath the covers, some type of
semantic relationship.

Now, you can go from 50,000 columns (dimensions in a vector space) to,
say 100 columns in which terms with similar meanings are collapsed
together.  Folks like this because it makes all that nasty ambiguity
and problems with synonyms go away.

You can now represent a document by the top dimensions in the reduced space.

I don't like LSA(LSI) because it is computationally intractable for
large document collections.  My approach does the same thing,
represent a document by fewer dimensions, each of which is a category,
but the categories are explicit.  Training is fast, categorization is
much, much faster.  Downside - some human must give you the sets of
categories and sample doucments to start; LSA just "learns" categories
along the way.

ALso, with LSI, you need to redo it every time you change the
documents in the collection (add 1 doc), but no one really does.

Semantic Web is next step up - humans tag documents with category ids.
I dont' like it because computers are here to serve us.  The effort
there is manually building the sets of categories.
Mom